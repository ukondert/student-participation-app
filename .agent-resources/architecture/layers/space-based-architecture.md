# Space-Based Architecture

> **Quelle:** Patterns for Scalable Systems  
> **Kategorie:** Architectural Pattern  
> **Ebene:** Gesamtarchitektur

---

## ğŸ¯ Ziel

Maximale Leistung und ElastizitÃ¤t bei ressourcenintensiven Prozessen durch Eliminierung zentraler FlaschenhÃ¤lse und durch verteilte Verarbeitung in einem "Space"-basierten Laufzeitmodell.

---

## ğŸ“‹ Architektur-Komponenten

| Komponente | Zweck | Artefakte | Hauptverantwortung |
|---------|-------|-----------|-------------------|
| **Processing Units (PUs)** | Arbeitsinstanzen innerhalb des Space | Service Instances, In-memory Caches | Verarbeitungslast verteilen |
| **Shared Nothing Space** | Verteilte Datenhaltung | In-Memory Grid, Data Partitions | Daten lokal halten, Partitioning |
| **Messaging / Queue** | Aufgabenverteilung | Task Queues, Event Channels | Work Distribution, Backpressure |
| **Data Grid / Cache** | Schneller Datenspeicher | In-Memory Data Grid (GigaSpaces, Hazelcast) | Niedrige Latenz, Skalierung |
| **Scaler / Orchestrator** | Skalierung und Placement | Scheduler, Auto-Scaler | Ressourcenverwaltung, Placement

---

## ğŸ—ï¸ Architektur-Diagramm

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     Client Layer     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Frontend/API  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            Space / Grid              â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ PU #1  â”‚  â”‚ PU #2  â”‚  â”‚ PU #3  â”‚  â”‚
        â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚     â”‚           â”‚           â”‚       â”‚
        â”‚  â”Œâ”€â”€â–¼â”€â”€â”     â”Œâ”€â”€â–¼â”€â”€â”     â”Œâ”€â”€â–¼â”€â”€â”    â”‚
        â”‚  â”‚Cacheâ”‚     â”‚Cache â”‚     â”‚Cache â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚Queue/Msg â”‚  â”‚DataGrid â”‚  â”‚Orchestr.â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Regel:** Keine zentralen Bottlenecks; Daten/Work lokal an PUs halten.

---

## âœ… Best Practices

| Regel | Beschreibung |
|-------|--------------|
| **Shared-Nothing** | Vermeide zentrale State-Locks und Shared DBs. |
| **Partitioning** | Daten und Arbeit partitionieren (sharding). |
| **Local Caching** | Reduziere Netzwerk-Latenz durch lokale Caches. |
| **Backpressure** | Messaging mit Backpressure zur Stabilisierung nutzen. |
| **Elastic Scaling** | Autoscaling von Processing Units nach Last. |
| **Replication** | Repliziere kritische Daten fÃ¼r Ausfallsicherheit. |

---

## ğŸ’» Beispiel: Verzeichnisstruktur / Deployment

```
space-based-system/
 â”œâ”€ processing-units/
 â”‚   â”œâ”€ pu-1/
 â”‚   â”œâ”€ pu-2/
 â”‚   â””â”€ pu-3/
 â”œâ”€ data-grid/
 â”‚   â””â”€ hazelcast-cluster/
 â”œâ”€ queue/
 â”‚   â””â”€ task-queue/
 â”œâ”€ orchestrator/
 â”‚   â””â”€ placement-service/
 â””â”€ infra/
     â”œâ”€ monitoring/
     â””â”€ autoscaler/
```

---

## ğŸ¤– KI-Agent Hinweise

* Verwende In-Memory-Grids (Hazelcast, GigaSpaces) fÃ¼r niedrige Latenz.
* Halte Daten und Arbeit lokal â€” prefer local reads over network.
* Nutze eventuelle persistenten Backing Stores nur fÃ¼r Durability, nicht fÃ¼r Hot Paths.
* Implementiere starke Observability (latency, queue depth, partition skew).
* Teste Partitioning-Strategien mit realen Lastprofilen.

---

## ğŸ“Œ Checkliste

- [ ] Shared-Nothing-Prinzip eingehalten
- [ ] Partitioning-Strategie definiert
- [ ] In-Memory-Data-Grid implementiert
- [ ] Backpressure im Messaging konfiguriert
- [ ] Autoscaling und Placement-Policy vorhanden
- [ ] Monitoring fÃ¼r queue-depth und partition-skew
- [ ] Failover/Replication getestet

---

## âš ï¸ Anti-Patterns vermeiden

âŒ **Zentrales Datenbank-Backbone**
```java
// FALSCH - Single DB wird Engpass
SELECT * FROM central_db.orders WHERE id = ?
```

âœ… **Local Partition & Cache**
```java
// RICHTIG - Local PU greift auf lokalen Cache
Order o = localCache.get(orderId);
if (o == null) { o = remoteStorage.fetch(orderId); localCache.put(orderId, o); }
```

âŒ **Unkontrolliertes Rebalancing**
```java
// FALSCH - StÃ¤ndiges Verschieben von Partitionen unter Last
rebalancer.runContinuously();
```

âœ… **Controlled Rebalancing**
```java
// RICHTIG - Rebalance nach stabilen Perioden
if (loadStable()) { rebalancer.rebalance(); }
```

---

## ğŸ”§ Technologie-Stack Beispiele

- GigaSpaces XAP
- Hazelcast IMDG
- Redis Cluster (fÃ¼r some use-cases)
- Kubernetes fÃ¼r managing PUs
- Kafka / Pulsar for messaging

---

## ğŸ“Š Wann Space-Based Architecture verwenden?

| âœ… Geeignet fÃ¼r | âŒ Nicht geeignet fÃ¼r |
|----------------|----------------------|
| Echtzeit-Spitzenlasten | Kleine CRUD-Apps |
| Hohe ParallelitÃ¤t & niedrige Latenz | Systeme ohne in-memory requirements |
| Numerische Berechnungen / HPC-Likes | Workloads mit zentralen Transaktionen |
| Systeme mit kurzfristig sehr hoher Last | Einfach zu wartende Monolithen |

---

## ğŸ”— Referenzen

- Related Patterns: Scale-Out, Distributed Caching
- Projekt-Beispiele: `docs/architecture/domain-categorization.md`
- GigaSpaces Documentation
- Hazelcast Documentation
